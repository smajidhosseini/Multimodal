{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings, os\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Input, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Flatten, concatenate, Dense\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Concatenate, Flatten, Dense, Reshape\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D,MaxPool1D, Flatten, Dense, Dropout\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers.pooling import MaxPooling2D, MaxPool1D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "path = '/nfsmount/majid/StressDetection/Hypothesis/Morteza/backup/Final'\n",
    "bio_file = 'Lasso_features_bio.csv'\n",
    "land_file = 'Lasso_features_land.csv'\n",
    "bio_label_file = 'data_scaled.individually_bio4.csv'\n",
    "land_label_file = 'data_scaled.individually_landmark4.csv'\n",
    "#import data\n",
    "bio_data = pd.read_csv(os.path.join(path,bio_file)).dropna(axis=0)\n",
    "land_data = pd.read_csv(os.path.join(path,land_file)).dropna(axis=0)\n",
    "bio_label = pd.read_csv(os.path.join(path,bio_label_file)).dropna(axis=0)\n",
    "land_label = pd.read_csv(os.path.join(path,land_label_file)).dropna(axis=0)\n",
    "#extract  labels and user IDs from another file\n",
    "bio_label = bio_label[['user','label']]\n",
    "land_label = land_label[['user','label']]\n",
    "\n",
    "bio_data = bio_data.join(bio_label)\n",
    "land_data = land_data.join(land_label)\n",
    "#Save the files for later runs\n",
    "#bio_data.to_csv('fetured_bio.csv',index=False)\n",
    "#land_data.to_csv('fetured_land.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#investigate data shapes to avoid mismatchs \n",
    "bio_data.columns ,bio_data.shape, land_data.columns ,land_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 10  # Height of your signal data (number of time steps)\n",
    "width = 10   # Width of your signal data (number of features)\n",
    "channels = 1 # Number of channels (e.g., 1 for grayscale)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Model.png](Model.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the multimodal late fusion model using keras\n",
    "input_bio = Input(shape=(30,))\n",
    "input_land = Input(shape=(height, width, channels))\n",
    "x_bio = Reshape((30, 1))(input_bio)\n",
    "x_bio = Conv1D(filters=32, kernel_size=3, activation='relu')(x_bio)\n",
    "x_bio = MaxPool1D(pool_size=2)(x_bio)\n",
    "x_bio = Flatten()(x_bio)\n",
    "x_bio = Dense(50, activation='relu')(x_bio)\n",
    "x_bio = Dense(25, activation='relu')(x_bio)\n",
    "output_bio = Dense(3, activation='softmax')(x_bio)\n",
    "x_land = Conv2D(32, kernel_size=(3, 3), activation='relu')(input_land)\n",
    "x_land = MaxPooling2D(pool_size=(2, 2))(x_land)\n",
    "x_land = Flatten()(x_land)\n",
    "x_land = Dense(50, activation='relu')(x_land)\n",
    "x_land = Dense(25, activation='relu')(x_land)\n",
    "output_land = Dense(0, activation='softmax')(x_land)\n",
    "\n",
    "# Concatenate the two output layers\n",
    "concatenated_output = Concatenate()([output_bio, output_land])\n",
    "# Create the late fusion model\n",
    "late_fusion_model = Model(inputs=[input_bio, input_land], outputs=concatenated_output)\n",
    "\n",
    "# Compile the model\n",
    "late_fusion_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "late_fusion_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leave one out cross validation using user ids from H to R\n",
    "epochs = 10\n",
    "\n",
    "for user in bio_data.user.unique():\n",
    "    print(user)\n",
    "    #Leave One subject Out Cross Validation train test split\n",
    "    train_bio, train_land = bio_data[bio_data['user']!=user], land_data[land_data['user']!=user]\n",
    "    test_bio,  test_land  = bio_data[bio_data['user']==user], land_data[land_data['user']==user]\n",
    "\n",
    "    #split labels and features\n",
    "    Xtrain_bio, Xtest_bio = train_bio.drop(columns=['user','label']), test_bio.drop(columns=['user','label'])\n",
    "    ytrain_bio, ytest_bio = train_bio['label'], test_bio['label']\n",
    "\n",
    "    \n",
    "    Xtrain_land = train_land.drop(columns=['user','label'])\n",
    "    #reshape training facial landmark data for conv2d\n",
    "    Xtrain_land = np.reshape(Xtrain_land.iloc[:, :100].values, (Xtrain_land.shape[0], height, width, channels))\n",
    "\n",
    "    ytrain_land = train_land['label'].values.astype(int)\n",
    "    encoder = LabelEncoder()\n",
    "    ytrain_land = encoder.fit_transform(ytrain_land)\n",
    "\n",
    "    Xtest_land = test_land.drop(columns=['user','label'])\n",
    "    Xtest_land = np.reshape(Xtest_land.iloc[:, :100].values, (Xtest_land.shape[0], height, width, channels))\n",
    "    \n",
    "    ytest_land = test_land['label']\n",
    "    ytest_land = ytest_land.values.astype(int)\n",
    "    encoder = LabelEncoder()\n",
    "    ytest_land = encoder.fit_transform(ytest_land)\n",
    "\n",
    "    #test station for debugging purposes\n",
    "    #print('Xtrain_bio.shape',Xtrain_bio.shape, 'ytrain_bio.shape',ytrain_bio.shape, 'Xtest_bio.shape',Xtest_bio.shape, 'ytest_bio.shape',ytest_bio.shape)\n",
    "    #print('Xtrain_land.shape',Xtrain_land.shape, 'ytrain_land.shape',ytrain_land.shape, 'Xtest_land.shape',Xtest_land.shape, 'ytest_land.shape',ytest_land.shape)\n",
    "\n",
    "    y_train = tf.keras.utils.to_categorical(ytrain_bio, num_classes=3)\n",
    "    y_test = tf.keras.utils.to_categorical(ytest_bio, num_classes=3)\n",
    # Compile the model\n",
    "late_fusion_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    late_fusion_model.fit([Xtrain_bio, Xtrain_land], y_train, epochs=epochs, batch_size=8, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict \n",
    "y_pred = late_fusion_model.predict([Xtest_bio, Xtest_land])\n",
    "y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TSP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
